Great question â€” and this gets to the core of how bandwidth (GB/s) requirements translate into capacity (TB per GPU) recommendations.

Letâ€™s walk through the logic step by step.

ðŸ§® Step 1: What Does 125â€¯GB/s per SU Mean?
A Scalable Unit (SU) = 32 DGX nodes = 256 GPUs

NVIDIAâ€™s baseline recommendation is 125â€¯GB/s read throughput per SU

That means each GPU, on average, consumes:

125
Â GB/s
256
Â GPUs
â‰ˆ
0.49
Â GB/sÂ perÂ GPU
256Â GPUs
125Â GB/s
â€‹
 â‰ˆ0.49Â GB/sÂ perÂ GPU
So, ~500 MB/s sustained per GPU.

This is bandwidth â€” now letâ€™s translate that into capacity.

ðŸ§® Step 2: Sustained Read Throughput Ã— Duration = Data Served
To size how much flash storage is needed, we ask:

"How much data must the system serve at that rate before it starts reusing old data, flushing, or reading from object tier?"

Letâ€™s assume:

A model training job runs for 1 hour (3600 seconds)

GPU keeps reading at 0.5â€¯GB/s (based on NVIDIAâ€™s 125â€¯GB/s SU spec)

Then per GPU data consumption over 1 hour is:

0.5
Â GB/s
Ã—
3600
Â s
=
1.8
Â TB
0.5Â GB/sÃ—3600Â s=1.8Â TB
But not all of this needs to be served from flash:

Some data may be cached or reused

Some datasets may be streamed once and discarded

Some jobs checkpoint and resume, not starting from scratch

So we apply a working set multiplier:

Assume 50â€“75% of data is unique / not cached

That gives you a realistic flash requirement of ~1â€“1.5â€¯TB per GPU

âœ… Final Translation
Parameter	Value
Sustained bandwidth per GPU	~0.5 GB/s
Training window	1â€“2 hours
Data read per GPU	1.8â€“3.6 TB
Flash storage required	1â€“1.5 TB per GPU (active working set)

So the 1â€“1.5 TB per GPU flash recommendation is about holding enough active dataset and checkpoint data to:

Avoid repeated remote/object access

Sustain high read throughput during training

Provide buffer space for multiple workloads

